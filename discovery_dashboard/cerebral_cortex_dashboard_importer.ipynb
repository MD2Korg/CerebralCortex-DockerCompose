{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery Dashboard Importer\n",
    "\n",
    "This data importer aims to help you import data from Cerebral Cortex into Discovery Dashboard. We offer convenient and flexible options for you to customize your dashboard.\n",
    "\n",
    "If you are not using Cerebral Cortex, you can also use this importer, but be sure to change the paths in the configuration section according to the comments.\n",
    "\n",
    "## Important\n",
    "\n",
    "- In order to generate meaningful and accurate visualizations, please edit the configurations according to your needs.\n",
    "    - Mistakes in configurations might introduce undesirable behaviors of the Dashboard.\n",
    "- Since not all data stored on Cerebral Cortex follow the same format (eg. data descriptors), some of the data might require some additional preprocessing in order to work. \n",
    "    - This importer is provided in the Jupyter Notebook so that you can easily preprocess your data :)\n",
    "\n",
    "## Concepts\n",
    "\n",
    "Discovery Dashboard is designed for studies with timeseries **measurements** and/or **events**.\n",
    "\n",
    "- **Stream**: A stream of time series, with a meaningful name\n",
    "    - **Measurements**: Streams with data points where each data point consists of a timestamp and a single scalar value. (eg. heart rate)\n",
    "    - **Events**: Streams with data points where each data point consists of a timestamp. (eg. smoking)\n",
    "    \n",
    "Notice that **stream** for the dashboard is more restrictive than CerebralCortex Core Library's datastream datatype. This is because fields like lists/JSON won't be directly visualized as line paths. If you have a datastream of tuple, consider separating them into multiple streams.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "If anything goes wrong, manually remove the named docker volume `cerebralcortexdockercompose_chronix_data` and restart the services.\n",
    "\n",
    "For more info about using `docker volume rm` see [docker's official documentation](https://docs.docker.com/engine/reference/commandline/volume_rm/)\n",
    "\n",
    "If using Cerebral Cortex Vagrant, remember to first `vagrant ssh` and then use docker related commands.\n",
    "\n",
    "## Under the hood\n",
    "\n",
    "Discovery Dashboard uses Chronix Server as time series storage. Chronix acceptes influx line protocol (with microsecond precision), which is used by the importer. You can easily write your own importer if you'd like to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample code to bridge your data to the importer\n",
    "\n",
    " - Converts data from Cerebral Cortex API\n",
    " - You can modify the code or just write your own\n",
    " - Code here is deliberately verbose for readability\n",
    " - If you don't have real data, but still want to test out the dashboard, scroll to the end of the notebook and use the *(Optional) Sample Data* Cell\n",
    " \n",
    "## Optional\n",
    " \n",
    " - The following example uses UUID. If you don't want to display the data anonymously, you can use the usernames instead.\n",
    "     - It's just a unique identifier to group the streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_NAME = 'demo'\n",
    "\n",
    "# For measurements, input lists of tuples (user_id, datetime, scalar_value)\n",
    "sample_data_battery_measurements = []\n",
    "\n",
    "# For events, input lists of tuples (user_id, datetime)\n",
    "sample_data_notification_post_events = []\n",
    "\n",
    "\n",
    "from cerebralcortex.cerebralcortex import CerebralCortex\n",
    "from datetime import datetime\n",
    "# Connect to CC Core API\n",
    "CC = CerebralCortex(\"../cc_config_file/cc_vagrant_configuration.yml\")\n",
    "# Get all users in the specified study\n",
    "users = CC.get_all_users(STUDY_NAME)\n",
    "\n",
    "for user in users:\n",
    "    user_id = user[\"identifier\"]\n",
    "    user_streams = CC.get_user_streams(user_id)\n",
    "    for _, val in user_streams.items():\n",
    "        # Here val is CC's datastream object\n",
    "        for stream_id in val[\"stream_ids\"]:\n",
    "            # Here are samples from cc_demo\n",
    "            # If you've walked through the other tutorials you might already\n",
    "            # have these sample data\n",
    "            if val[\"name\"] == \"CU_NOTIF_POST_PACKAGE--org.md2k.mcerebrum\":\n",
    "                data_stream = CC.get_stream(stream_id=stream_id, day=CC.get_stream_days(stream_id)[0])\n",
    "                for data_point in data_stream.data:\n",
    "                    sample_data_notification_post_events.append((user_id, data_point.start_time))\n",
    "            if val[\"name\"] == \"BATTERY--org.md2k.mcerebrum--PHONE\":\n",
    "                data_stream = CC.get_stream(stream_id=stream_id, day=CC.get_stream_days(stream_id)[0])\n",
    "                for data_point in data_stream.data:\n",
    "                    sample_data_battery_measurements.append((user_id, data_point.start_time, data_point.sample[1]))\n",
    "                \n",
    "sample_data_notification_post_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations\n",
    "\n",
    "- Modify the config below and run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importer_config = {\n",
    "#--------------------------------------\n",
    "# URL to Dashboard server and database\n",
    "# If using Cerebral Cortex, use the following two lines:\n",
    "    \"server_endpoint\": \"http://dashboardserver:4567/api/config\",\n",
    "    \"chronix_endpoint\": \"http://chronix:8983/solr/chronix/ingest/influxdb/write\",\n",
    "# If running locally via docker on mac, linux, or Windows 10 with Hyper V:\n",
    "#     \"server_endpoint\": \"http://localhost:4567/api/config\",\n",
    "#     \"chronix_endpoint\": \"http://localhost:8983/solr/chronix/ingest/influxdb/write\",\n",
    "# If running locally via Docker Toolbox on Windows:\n",
    "#     \"server_endpoint\": \"http://192.168.99.100:4567/api/config\",\n",
    "#     \"chronix_endpoint\": \"http://192.168.99.100:8983/solr/chronix/ingest/influxdb/write\",\n",
    "#---------------------------------------\n",
    "# Name of the study that you want to visualize\n",
    "    \"study_name\": \"Demo\",\n",
    "# Stream Names\n",
    "# - Dictionary key is whatever name you want to display\n",
    "# - Dictionary value is a list\n",
    "#   - For \"measurements\", the list contains tuples of (user_id, datetime, scalar_value)\n",
    "#   - For \"events\", the list contains tuples of (user_id, datetime)\n",
    "    \"measurements\": {\n",
    "        \"battery\": sample_data_battery_measurements,\n",
    "        \"temperature\": example_measurements_2\n",
    "    },\n",
    "    \"events\": {\n",
    "        \"notification\": sample_data_notification_post_events,\n",
    "        \"sms\": example_events_2\n",
    "    },\n",
    "# The time series resolutions that you want to use for resampling\n",
    "# [\"high frequency\", \"low frequency\"] using pandas notation https://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n",
    "# These will be displayed in and out of zoomed lens respectively\n",
    "# Commonly [\"1Min\", \"5Min\"]\n",
    "    \"resolutions\": [\"1Min\", \"5Min\"],\n",
    "# Specify a timezone that you want to display the data in\n",
    "# The timezone string needs to be in the format of IANA Timezone database\n",
    "# eg. \"Asia/Shanghai\", \"America/New_York\", \"UTC\"\n",
    "# Warning: If the timezone is not set correctly, the browser can demonstrate unexpected behaviors\n",
    "    \"timezone\": \"America/New_York\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importer\n",
    "\n",
    "- Run the cell below to start import\n",
    "- You don't need to modify this, though you can write your own importer if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Code for the importer itself\n",
    "# \n",
    "# Don't modify unless you know what you are doing\n",
    "# -------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def format_tags(tags):\n",
    "    return ','.join('{}={}'.format(k, v) for k, v in tags.items())\n",
    "\n",
    "def format_name(tags):\n",
    "    return '-'.join('{}'.format(v) for _, v in tags.items())\n",
    "\n",
    "def format_influx_line_protocol(measurement, tags, value,\n",
    "                                ts):\n",
    "    return '{},{} {} {}'.format(measurement, tags, 'value={}'.format(value),\n",
    "                                ts)\n",
    "\n",
    "def import_into_dashboard(**kwargs):\n",
    "    configs = {}\n",
    "    # Required by importer\n",
    "    configs[\"server_endpoint\"] = kwargs[\"server_endpoint\"]\n",
    "    configs[\"chronix_endpoint\"] = kwargs[\"chronix_endpoint\"]\n",
    "    configs[\"study_name\"] = kwargs[\"study_name\"]\n",
    "    configs[\"measurements\"] = list(kwargs[\"measurements\"].keys())\n",
    "    configs[\"events\"] = list(kwargs[\"events\"].keys())\n",
    "    configs[\"resolutions\"] = kwargs[\"resolutions\"]\n",
    "    configs[\"timezone\"] = kwargs[\"timezone\"]\n",
    "    measurements = kwargs[\"measurements\"]\n",
    "    events = kwargs[\"events\"]\n",
    "    # Optional Customizations\n",
    "    configs[\"title\"] = kwargs.get(\"title\", \"Discovery Dashboard: \" + configs[\"study_name\"])\n",
    "    \n",
    "    # Import configs to dashboard server\n",
    "    r = requests.post(configs['server_endpoint'], data=json.dumps(configs))\n",
    "    print(r.text)\n",
    "    \n",
    "    # Import events\n",
    "    for stream_name, event_stream in events.items():\n",
    "        request_buffer = []\n",
    "        for (user_id, pydatetime) in event_stream:\n",
    "            data_dict = {\n",
    "                'studyName': configs[\"study_name\"],\n",
    "                'streamName': stream_name,\n",
    "                'groupById': user_id,\n",
    "                'metricType': 'event',\n",
    "                'type': 'metric',\n",
    "            }\n",
    "            request_buffer.append(format_influx_line_protocol(\n",
    "                format_name(data_dict),\n",
    "                format_tags(data_dict),\n",
    "                1,\n",
    "                int(pydatetime.timestamp() * 1000)\n",
    "            ))\n",
    "        r = requests.post(configs[\"chronix_endpoint\"], data='\\n'.join(request_buffer))\n",
    "        print(r.text)\n",
    "    \n",
    "    \n",
    "    # Import measurements\n",
    "    for stream_name, measurement_stream in measurements.items():\n",
    "        df = pd.DataFrame(measurement_stream, columns=['user_id', 'timestamp', 'val']).set_index(['timestamp'])\n",
    "        for resolution in configs[\"resolutions\"]:\n",
    "            request_buffer = []\n",
    "            df_res = df.groupby(['user_id']).resample(resolution).mean().fillna(method='pad', limit=1).fillna(-1).reset_index()\n",
    "            for (user_id, timestamp, value) in [(val[0], int(val[1].to_pydatetime().timestamp() * 1000), val[2]) for val in df_res.values]:\n",
    "                data_dict = {\n",
    "                    'studyName': configs[\"study_name\"],\n",
    "                    'streamName': stream_name,\n",
    "                    'resolution': resolution,\n",
    "                    'groupById': user_id,\n",
    "                    'metricType': 'measurement',\n",
    "                    'type': 'metric',\n",
    "                }\n",
    "                tags_str = format_tags(data_dict)\n",
    "                name_str = format_name(data_dict)\n",
    "                request_buffer.append(format_influx_line_protocol(name_str, tags_str, value, timestamp))\n",
    "            r = requests.post(configs[\"chronix_endpoint\"], data='\\n'.join(request_buffer))\n",
    "            print(r.text)\n",
    "\n",
    "\n",
    "import_into_dashboard(**importer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Sample Data\n",
    "\n",
    "- If you don't have real data, but just want to test out the dashboard\n",
    "- Run the cell below to generate some random dummy sample data\n",
    "- Remember to plug them into the config cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "user_id_list = [str(uuid.uuid4()) for i in range(20)]\n",
    "\n",
    "measurements_stream_1 = []\n",
    "measurements_stream_2 = []\n",
    "events_stream_1 = []\n",
    "events_stream_2 = []\n",
    "\n",
    "for user_id in user_id_list:\n",
    "    measurements_stream_1.extend([(user_id, datetime.datetime.now() + datetime.timedelta(seconds=(60 * i)), random.random()) for i in range(60 * 24 * 3)])\n",
    "    measurements_stream_2.extend([(user_id, datetime.datetime.now() + datetime.timedelta(seconds=(60 * i)), random.random()) for i in range(60 * 24 * 3)])\n",
    "    events_stream_1.extend([(user_id, datetime.datetime.now() + datetime.timedelta(seconds=(3600 * i))) for i in range(48)])\n",
    "    events_stream_2.extend([(user_id, datetime.datetime.now() + datetime.timedelta(seconds=(3600 * i))) for i in range(72)])\n",
    "\n",
    "sample_data_battery_measurements = measurements_stream_1\n",
    "sample_data_notification_post_events = events_stream_1\n",
    "example_measurements_2 = measurements_stream_2\n",
    "example_events_2 = events_stream_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
